{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors, metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.utils import resample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"\\UAIS\\datathon\\CAW\\train.csv\")\n",
    "test = pd.read_csv(r\"\\UAIS\\datathon\\CAW\\test.csv\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['size_class'] = train['size_class'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(val):\n",
    "    stripped_val = str(val).strip()\n",
    "    if stripped_val == \"Surface\":\n",
    "        return \"Surface\"\n",
    "    elif stripped_val == \"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "\n",
    "train['fire_type'] = train['fire_type'].apply(replace_values)\n",
    "test['fire_type'] = test['fire_type'].apply(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Letter_fuel'] = train['fuel_type'].str[0]\n",
    "test['Letter_fuel'] = test['fuel_type'].str[0]\n",
    "\n",
    "train['temperature'] = train['temperature'] + 33\n",
    "test['temperature'] = test['temperature'] + 33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region(fire_number):\n",
    "\n",
    "    forest_areas = {\n",
    "        'C': 'Calgary',\n",
    "        'E': 'Edson',\n",
    "        'H': 'High Level',\n",
    "        'G': 'Grande Prairie',\n",
    "        'L': 'Lac La Biche',\n",
    "        'M': 'Fort McMurray',\n",
    "        'P': 'Peace River',\n",
    "        'R': 'Rocky',\n",
    "        'S': 'Slave Lake',\n",
    "        'W': 'Whitecourt'\n",
    "    }\n",
    "    \n",
    "\n",
    "    region = forest_areas.get(fire_number[0], \"Unknown\")\n",
    "    \n",
    "    return region\n",
    "\n",
    "train['region'] = train['fire_number'].apply(extract_region)\n",
    "test['region'] = test['fire_number'].apply(extract_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_name'] = train['fire_name'].notnull().astype(int)\n",
    "test['has_name'] = test['fire_name'].notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bucketing_on_fire'] = train['bucketing_on_fire'].str.lower()\n",
    "test['bucketing_on_fire'] = test['bucketing_on_fire'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_reasonable_date = pd.Timestamp('2000-01-01')\n",
    "max_reasonable_date = pd.Timestamp('2023-12-31')\n",
    "\n",
    "\n",
    "date_features = ['fire_start_date', 'discovered_date', 'reported_date', 'dispatch_date','start_for_fire_date', 'assessment_datetime', 'ia_arrival_at_fire_date', 'fire_fighting_start_date', 'first_bucket_drop_date', 'ex_fs_date']\n",
    "\n",
    "for feature in date_features:\n",
    "\n",
    "    train[feature] = pd.to_datetime(train[feature], errors='coerce')\n",
    "    test[feature] = pd.to_datetime(test[feature], errors='coerce')\n",
    "\n",
    "\n",
    "    train.loc[(train[feature] < min_reasonable_date) | (train[feature] > max_reasonable_date), feature] = pd.NaT\n",
    "    test.loc[(test[feature] < min_reasonable_date) | (test[feature] > max_reasonable_date), feature] = pd.NaT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mapping = {\n",
    "    'Clear': 0,\n",
    "    'CB dry': 1,\n",
    "    'Cloudy': 2,\n",
    "    'CB wet': 3,\n",
    "    'Rain showers': 4\n",
    "}\n",
    "\n",
    "train['weather_conditions_encoded'] = train['weather_conditions_over_fire'].map(weather_mapping)\n",
    "test['weather_conditions_encoded'] = test['weather_conditions_over_fire'].map(weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"fire_name\", \"fire_number\", \"discovered_size\", \"industry_identifier_desc\", \"weather_conditions_over_fire\"]\n",
    "\n",
    "train = train.drop(columns=drop_list)\n",
    "test = test.drop(columns=drop_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_features(df): \n",
    "\n",
    "    def timedelta_to_minutes(td):\n",
    "        return td.total_seconds() / 60 if pd.notna(td) else np.nan\n",
    "\n",
    "\n",
    "    def compute_time_difference(start, end):\n",
    "        if pd.isna(start) or pd.isna(end):\n",
    "            return np.nan\n",
    "        return timedelta_to_minutes(end - start)\n",
    "\n",
    "    df['time_to_discovery'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['discovered_date']), axis=1)\n",
    "    df['time_to_report'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['reported_date']), axis=1)\n",
    "    df['time_to_dispatch'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['dispatch_date']), axis=1)\n",
    "    df['time_to_start_for_fire'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['start_for_fire_date']), axis=1)\n",
    "    df['time_to_assessment'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['assessment_datetime']), axis=1)\n",
    "    df['time_to_ia_arrival'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['ia_arrival_at_fire_date']), axis=1)\n",
    "    df['time_to_start_fighting'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['fire_fighting_start_date']), axis=1)\n",
    "    df['time_to_first_bucket_drop'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['first_bucket_drop_date']), axis=1)\n",
    "    df['total_time_to_extinguish'] = df.apply(lambda row: compute_time_difference(row['fire_start_date'], row['ex_fs_date']), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = generate_time_features(train)\n",
    "test = generate_time_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_datelist = ['fire_start_date', 'discovered_date', 'reported_date', 'dispatch_date','start_for_fire_date', 'assessment_datetime', 'ia_arrival_at_fire_date', 'fire_fighting_start_date', 'first_bucket_drop_date', 'ex_fs_date']\n",
    "\n",
    "train = train.drop(columns=drop_datelist)\n",
    "test = test.drop(columns=drop_datelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_list = list(train.columns)\n",
    "test_feature_list = list(test.columns)\n",
    "label_size_class = LabelEncoder()\n",
    "\n",
    "for feature in train_feature_list:\n",
    " \n",
    "    if train[feature].dtype == 'object':\n",
    "        not_null_mask = train[feature].notnull() \n",
    "        train.loc[not_null_mask, feature] = label_size_class.fit_transform(train.loc[not_null_mask, feature])\n",
    "\n",
    "for feature in test_feature_list:\n",
    "\n",
    "    if test[feature].dtype == 'object':\n",
    "        not_null_mask = test[feature].notnull()\n",
    "        test.loc[not_null_mask, feature] = label_size_class.fit_transform(test.loc[not_null_mask, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gmm_class_feature(feat, n):\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "    gmm.fit(train[feat].values.reshape(-1, 1))\n",
    "    train[f'{feat}_class'] = gmm.predict(train[feat].values.reshape(-1, 1))\n",
    "    test[f'{feat}_class'] = gmm.predict(test[feat].values.reshape(-1, 1))\n",
    "\n",
    "get_gmm_class_feature(\"fire_location_latitude\", 2)\n",
    "get_gmm_class_feature(\"fire_location_longitude\", 2)\n",
    "get_gmm_class_feature(\"assessment_hectares\", 6)\n",
    "get_gmm_class_feature(\"fire_spread_rate\", 8)\n",
    "get_gmm_class_feature(\"temperature\", 10)\n",
    "get_gmm_class_feature(\"relative_humidity\", 15)\n",
    "get_gmm_class_feature(\"wind_speed\", 5)\n",
    "get_gmm_class_feature(\"fire_fighting_start_size\", 19)\n",
    "get_gmm_class_feature(\"distance_from_water_source\", 19)\n",
    "get_gmm_class_feature(\"time_to_discovery\", 6)\n",
    "get_gmm_class_feature(\"time_to_report\", 19)\n",
    "get_gmm_class_feature(\"time_to_dispatch\", 7)\n",
    "get_gmm_class_feature(\"time_to_start_for_fire\", 13)\n",
    "get_gmm_class_feature(\"time_to_assessment\", 2)\n",
    "get_gmm_class_feature(\"time_to_ia_arrival\", 7)\n",
    "get_gmm_class_feature(\"time_to_start_fighting\", 6)\n",
    "get_gmm_class_feature(\"time_to_first_bucket_drop\", 10)\n",
    "get_gmm_class_feature(\"total_time_to_extinguish\", 17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def get_gmm_class_feature(feat, n):\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "    gmm.fit(train[feat].values.reshape(-1, 1))\n",
    "    train[f'{feat}_class'] = gmm.predict(train[feat].values.reshape(-1, 1))\n",
    "    test[f'{feat}_class'] = gmm.predict(test[feat].values.reshape(-1, 1))\n",
    "\n",
    "def evaluate_classifier():\n",
    "    model1 = CatBoostClassifier(verbose=0)\n",
    "    model1.fit(train.drop(columns='size_class'), train['size_class'])\n",
    "    y_pred1 = model1.predict(test.drop(columns='size_class'))\n",
    "    return balanced_accuracy_score(test['size_class'], y_pred1)\n",
    "\n",
    "features_to_optimize = [\n",
    "    \"fire_location_latitude\",\n",
    "    \"fire_location_longitude\",\n",
    "    \"assessment_hectares\",\n",
    "    \"fire_spread_rate\",\n",
    "    \"temperature\",\n",
    "    \"relative_humidity\",\n",
    "    \"wind_speed\",\n",
    "    \"fire_fighting_start_size\",\n",
    "    \"distance_from_water_source\",\n",
    "    \"time_to_discovery\",\n",
    "    \"time_to_report\",\n",
    "    \"time_to_dispatch\",\n",
    "    \"time_to_start_for_fire\",\n",
    "    \"time_to_assessment\",\n",
    "    \"time_to_ia_arrival\",\n",
    "    \"time_to_start_fighting\",\n",
    "    \"time_to_first_bucket_drop\",\n",
    "    \"total_time_to_extinguish\"\n",
    "]\n",
    "\n",
    "optimal_components = {}\n",
    "\n",
    "for feat in features_to_optimize:\n",
    "    best_score = -np.inf\n",
    "    best_components = None\n",
    "    \n",
    "    for n in range(2, 21):\n",
    "        get_gmm_class_feature(feat, n)\n",
    "        score = evaluate_classifier()\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_components = n\n",
    "    \n",
    "    optimal_components[feat] = best_components\n",
    "    print(f\"Optimal components for {feat}: {best_components}, Balanced Accuracy: {best_score:.4f}\")\n",
    "\n",
    "print(\"All optimal components:\", optimal_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"fire_id\"]\n",
    "\n",
    "X = train.drop(cols_to_drop + [\"size_class\"], axis=1)\n",
    "y = train[\"size_class\"]\n",
    "\n",
    "X_train_original, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = pd.concat([X_train_original, y_train], axis=1)\n",
    "\n",
    "max_size = train_data[\"size_class\"].value_counts().max()\n",
    "\n",
    "lst = [train_data]\n",
    "for class_index, group in train_data.groupby(\"size_class\"):\n",
    "    if len(group) < max_size:\n",
    "        lst.append(resample(group, replace=True, n_samples=max_size-len(group)))\n",
    "\n",
    "train_data_oversampled = pd.concat(lst)\n",
    "train_data_oversampled = train_data_oversampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train_oversampled = train_data_oversampled.drop(\"size_class\", axis=1)\n",
    "y_train = train_data_oversampled[\"size_class\"]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_oversampled)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model_eval, pbounds, init_points=5, n_iter=10):\n",
    "    optimizer = BayesianOptimization(f=model_eval, pbounds=pbounds, random_state=42)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    best_params = optimizer.max['params']\n",
    "    for key, val in best_params.items():\n",
    "        if isinstance(val, float) and val.is_integer():\n",
    "            best_params[key] = int(val)\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_eval(learning_rate, depth, l2_leaf_reg):\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'depth': int(depth),\n",
    "        'l2_leaf_reg': l2_leaf_reg,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    clf = CatBoostClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = balanced_accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "bounds_catboost = {\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'depth': (4, 8),\n",
    "    'l2_leaf_reg': (1, 5),\n",
    "}\n",
    "best_params_catboost = optimize_model(catboost_eval, bounds_catboost)\n",
    "best_params_catboost['depth'] = int(best_params_catboost['depth'])\n",
    "model1 = CatBoostClassifier(**best_params_catboost)\n",
    "model1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_eval(learning_rate, max_depth, gamma, colsample_bytree):\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree\n",
    "    }\n",
    "    clf = XGBClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = balanced_accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "bounds_xgboost = {\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.5, 1)\n",
    "}\n",
    "best_params_xgboost = optimize_model(xgb_eval, bounds_xgboost)\n",
    "best_params_xgboost['max_depth'] = int(best_params_xgboost['max_depth'])\n",
    "\n",
    "model2 = XGBClassifier(**best_params_xgboost)\n",
    "model2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_eval(n_estimators, max_depth, min_samples_split):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'max_depth': int(max_depth),\n",
    "        'min_samples_split': int(min_samples_split),\n",
    "    }\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = balanced_accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "bounds_rf = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'max_depth': (5, 40),\n",
    "    'min_samples_split': (2, 20)\n",
    "}\n",
    "\n",
    "best_params_rf = optimize_model(rf_eval, bounds_rf)\n",
    "best_params_rf['n_estimators'] = int(best_params_rf['n_estimators'])\n",
    "best_params_rf['max_depth'] = int(best_params_rf['max_depth'])\n",
    "best_params_rf['min_samples_split'] = int(best_params_rf['min_samples_split'])\n",
    "\n",
    "model3 = RandomForestClassifier(**best_params_rf)\n",
    "model3.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lgbm_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "    params = {\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'max_depth': int(max_depth),\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_split_gain': min_split_gain,\n",
    "        'min_child_weight': min_child_weight,\n",
    "    }\n",
    "    clf = LGBMClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = balanced_accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "bounds_lgbm = {\n",
    "    'num_leaves': (31, 500),\n",
    "    'feature_fraction': (0.1, 0.9),\n",
    "    'bagging_fraction': (0.8, 1),\n",
    "    'max_depth': (5, 8.99),\n",
    "    'lambda_l1': (0, 5),\n",
    "    'lambda_l2': (0, 3),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'min_child_weight': (5, 50),\n",
    "}\n",
    "best_params_lgbm = optimize_model(lgbm_eval, bounds_lgbm)\n",
    "best_params_lgbm['max_depth'] = int(best_params_lgbm['max_depth'])\n",
    "best_params_lgbm['num_leaves'] = int(best_params_lgbm['num_leaves'])\n",
    "\n",
    "model4 = LGBMClassifier(**best_params_lgbm)\n",
    "model4.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict(X_test)\n",
    "print(\"CatBoost Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred1))\n",
    "\n",
    "y_pred2 = model2.predict(X_test)\n",
    "print(\"XGBoost Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred2))\n",
    "\n",
    "y_pred3 = model3.predict(X_test)\n",
    "print(\"RandomForest Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred3))\n",
    "\n",
    "\n",
    "y_pred4 = model4.predict(X_test)\n",
    "print(\"LightGBM Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred4))\n",
    "\n",
    "weights = [0.25, 0.25, 0.0, 0.5] \n",
    "\n",
    "probs1 = model1.predict_proba(X_test)\n",
    "probs2 = model2.predict_proba(X_test)\n",
    "probs3 = model3.predict_proba(X_test)\n",
    "probs4 = model4.predict_proba(X_test)\n",
    "    \n",
    "final_probs = (weights[0] * probs1 + weights[1] * probs2 + \n",
    "               weights[2] * probs3 + weights[3] * probs4)\n",
    "\n",
    "final_probs[:, 3] += 0.0 * final_probs[:, 2]\n",
    "final_probs[:, 2] -= 0.0 * final_probs[:, 2]\n",
    "\n",
    "\n",
    "final_probs /= final_probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "final_pred = np.argmax(final_probs, axis=1)\n",
    "\n",
    "ensemble_balanced_accuracy = balanced_accuracy_score(y_test, final_pred)\n",
    "print(\"Ensemble Balanced Accuracy with Weighted Soft Voting after adjustment:\", ensemble_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_1d = np.ravel(y_test)\n",
    "#final_pred_1d = np.ravel(final_pred) \n",
    "\n",
    "#mismatches = y_test_1d != final_pred_1d\n",
    "\n",
    "#mismatched_probs = final_probs[mismatches]\n",
    "\n",
    "#mismatched_data = pd.DataFrame({\n",
    "#    'fire_id': test.loc[mismatches, 'fire_id'],\n",
    "#    'Actual_size_class': y_test_1d[mismatches],\n",
    "#    'Predicted': final_pred_1d[mismatches],\n",
    "#    'Prob_Class_0': mismatched_probs[:, 0],\n",
    "#    'Prob_Class_1': mismatched_probs[:, 1],\n",
    "#    'Prob_Class_2': mismatched_probs[:, 2],\n",
    "#    'Prob_Class_3': mismatched_probs[:, 3],\n",
    "#    'Prob_Class_4': mismatched_probs[:, 4]\n",
    "#})\n",
    "\n",
    "\n",
    "#mismatched_data.to_csv('mismatched_data_with_probs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, final_pred))\n",
    "print(confusion_matrix(y_test, final_pred))\n",
    "ensemble_accuracy = accuracy_score(y_test, final_pred)\n",
    "print(ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model1.predict_proba(X_test)\n",
    "print(probabilities)\n",
    "df_probabilities = pd.DataFrame(probabilities)\n",
    "df_probabilities.to_csv('probabilities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real = test.drop(cols_to_drop, axis=1)\n",
    "X_test_real_transformed = sc.transform(X_test_real)\n",
    "\n",
    "y_pred1_real = model1.predict(X_test_real_transformed)\n",
    "y_pred2_real = model2.predict(X_test_real_transformed)\n",
    "y_pred3_real = model3.predict(X_test_real_transformed)\n",
    "y_pred4_real = model4.predict(X_test_real_transformed)\n",
    "\n",
    "weights = [0.25, 0.25, 0.0, 0.5] \n",
    "\n",
    "probs1_real = model1.predict_proba(X_test_real_transformed)\n",
    "probs2_real = model2.predict_proba(X_test_real_transformed)\n",
    "probs3_real = model3.predict_proba(X_test_real_transformed)\n",
    "probs4_real = model4.predict_proba(X_test_real_transformed)\n",
    "    \n",
    "final_probs_real = (weights[0] * probs1_real + weights[1] * probs2_real + \n",
    "               weights[2] * probs3_real + weights[3] * probs4_real)\n",
    "\n",
    "final_probs_real[:, 3] += 0.0 * final_probs_real[:, 2]\n",
    "final_probs_real[:, 2] -= 0.0 * final_probs_real[:, 2]\n",
    "\n",
    "\n",
    "final_probs_real /= final_probs_real.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "final_pred_real = np.argmax(final_probs_real, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'fire_id': test.index, 'size_class': final_pred_real+1})\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
